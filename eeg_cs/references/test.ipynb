{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def somp(Y: np.ndarray, Phi: np.ndarray, s: int):\n",
    "    \"\"\"\n",
    "    Simultaneous Orthogonal Matching Pursuit (SOMP)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y   : array, shape (m, K)\n",
    "          Matrix of K measurement vectors (each column is one measurement).\n",
    "    Phi : array, shape (m, n)\n",
    "          Dictionary matrix whose columns are the atoms.\n",
    "    s   : int\n",
    "          Sparsity level: number of atoms to select.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    S   : list of int\n",
    "          Indices of the selected atoms (size s).\n",
    "    X   : array, shape (s, K)\n",
    "          Coefficient matrix (only the rows corresponding to S are nonzero).\n",
    "    \"\"\"\n",
    "    # Initialize residual and support\n",
    "    R = Y.copy()                    # residual: shape (m, K)\n",
    "    S = []                          # support set (list of selected indices)\n",
    "    \n",
    "    # Precompute Phi^T for efficiency\n",
    "    PhiT = Phi.T                  # shape (n, m)\n",
    "    \n",
    "    for _ in range(s):\n",
    "        # 1) Compute (absolute) correlations for each atom j: sum over all K signals\n",
    "        #    corr[j] = || (Phi[:, j].T @ R) ||_1\n",
    "        # Using matrix form: (Phi^T @ R) is (n x K), then take abs and sum across columns.\n",
    "        corr = np.sum(np.abs(PhiT @ R), axis=1)  # shape (n,)\n",
    "        \n",
    "        # 2) Mask out already-selected atoms (set their score to -inf)\n",
    "        mask = np.ones_like(corr, dtype=bool)\n",
    "        mask[S] = False\n",
    "        corr_masked = np.where(mask, corr, -np.inf)\n",
    "        \n",
    "        # 3) Pick the atom with maximum correlation\n",
    "        j_best = int(np.argmax(corr_masked))\n",
    "        S.append(j_best)\n",
    "        \n",
    "        # 4) Form the sub-dictionary with selected atoms\n",
    "        Phi_S = Phi[:, S]           # shape (m, |S|)\n",
    "        \n",
    "        # 5) Solve least-squares to get coefficients for all K signals:\n",
    "        #    X_S = Phi_S^+ Y, where Phi_S^+ is the Moore–Penrose pseudoinverse\n",
    "        X_S = np.linalg.pinv(Phi_S) @ Y  # shape (|S|, K)\n",
    "        \n",
    "        # 6) Update residuals: R = Y - Phi_S X_S\n",
    "        R = Y - Phi_S @ X_S           # shape (m, K)\n",
    "    \n",
    "    # X: full n×K coefficient matrix, but only rows in S are nonzero.\n",
    "    X = np.zeros((Phi.shape[1], Y.shape[1]))\n",
    "    X[S, :] = X_S\n",
    "    \n",
    "    return S, X\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # synthetic demo\n",
    "#     m, n, K, s = 50, 100, 3, 5\n",
    "#     np.random.seed(0)\n",
    "#     Phi = np.random.randn(m, n)\n",
    "#     true_support = np.random.choice(n, s, replace=False)\n",
    "#     X_true = np.zeros((n, K))\n",
    "#     X_true[true_support] = np.random.randn(s, K)\n",
    "#     Y = Phi @ X_true + 0.01 * np.random.randn(m, K)\n",
    "    \n",
    "#     S_est, X_est = somp(Y, Phi, s)\n",
    "#     # print(\"True support:     \", sorted(true_support))\n",
    "#     # print(\"Estimated support:\", sorted(S_est))\n",
    "#     print(X_true)\n",
    "#     print(X_est)\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.fftpack import idct\n",
    "# from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "\n",
    "# # Set random seed for reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # Parameters\n",
    "# N = 256        # signal dimension\n",
    "# M = 80         # number of measurements\n",
    "# s = 15         # sparsity level in DCT domain for OMP\n",
    "\n",
    "# # Time vector\n",
    "# t = np.linspace(0, 1, N, endpoint=False)\n",
    "\n",
    "# # Generate signal: sum of three sine waves\n",
    "# frequencies = [5, 20, 50]  # in cycles per interval\n",
    "# x = sum(np.sin(2 * np.pi * f * t) for f in frequencies)\n",
    "\n",
    "# # Build random Gaussian sensing matrix\n",
    "# Phi = np.random.randn(M, N)\n",
    "\n",
    "# # Take measurements\n",
    "# y = Phi.dot(x)\n",
    "\n",
    "# # Build inverse-DCT matrix and sensing dictionary\n",
    "# B = idct(np.eye(N), norm='ortho')  # maps DCT coefficients to time domain\n",
    "# Theta = Phi.dot(B)\n",
    "\n",
    "# # Reconstruct sparse DCT coefficients via OMP\n",
    "# omp = OrthogonalMatchingPursuit(n_nonzero_coefs=s)\n",
    "# omp.fit(Theta, y)\n",
    "# alpha_hat = omp.coef_\n",
    "\n",
    "# # Recover the time-domain signal\n",
    "# x_hat = idct(alpha_hat, norm='ortho')\n",
    "\n",
    "# # Plot original vs. reconstructed signals\n",
    "# plt.figure()\n",
    "# plt.plot(t, x)\n",
    "# plt.plot(t, x_hat)\n",
    "# plt.title('Original (sum of 3 sines) vs. Reconstructed Signal')\n",
    "# plt.legend(['Original', 'Reconstructed'])\n",
    "# plt.show()\n",
    "\n",
    "# # Print recovered sparsity support\n",
    "# print(\"Recovered nonzero DCT indices:\", np.nonzero(alpha_hat)[0])\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# from scipy.fftpack import dct, idct\n",
    "\n",
    "# # 1. Sample signal\n",
    "# n = 1024\n",
    "# x = np.sin(np.linspace(0, 4*np.pi, n))\n",
    "\n",
    "# # 2. Measurement matrix\n",
    "# m = int(0.3 * n)\n",
    "# Phi = np.random.randn(m, n)\n",
    "# Phi /= np.linalg.norm(Phi, axis=1, keepdims=True)\n",
    "\n",
    "# # 3. Forward and adjoint operators\n",
    "# def A_dct(s):\n",
    "#     \"\"\"y = Φ Ψ s  with Ψ = IDCT\"\"\"\n",
    "#     x_rec = idct(s, norm='ortho')      # inverse‐DCT synthesis\n",
    "#     return Phi @ x_rec\n",
    "\n",
    "# def At_dct(r):\n",
    "#     \"\"\"Ψᵀ Φᵀ r  with Ψᵀ = DCT\"\"\"\n",
    "#     temp = Phi.T @ r\n",
    "#     return dct(temp, norm='ortho')     # forward‐DCT analysis\n",
    "\n",
    "# # 4. Soft‐threshold\n",
    "# def soft(u, thresh):\n",
    "#     return np.sign(u) * np.maximum(np.abs(u) - thresh, 0)\n",
    "\n",
    "# # 5. ISTA\n",
    "# def ista_dct(y, lam, tau, max_iter=100):\n",
    "#     s = np.zeros(n)\n",
    "#     for _ in range(max_iter):\n",
    "#         grad = At_dct(A_dct(s) - y)\n",
    "#         s = soft(s - tau*grad, lam*tau)\n",
    "#     return s\n",
    "\n",
    "# # 6. Run CS + DCT\n",
    "# y = Phi @ x\n",
    "# L = np.linalg.norm(Phi, 2)**2    # Lipschitz approx\n",
    "# tau = 1.0 / L\n",
    "# lam = 0.01\n",
    "\n",
    "# s_hat = ista_dct(y, lam, tau, max_iter=200)\n",
    "# x_hat = idct(s_hat, norm='ortho')\n",
    "\n",
    "# print(\"NMSE:\", np.linalg.norm(x - x_hat)**2 / np.linalg.norm(x - x.mean())**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "s = 1\n",
    "B = 2\n",
    "alpha = 0.5 * np.log(0.5*(B + 1/B))\n",
    "\n",
    "# 4 * s * B * np.sqrt(2*alpha / np.pi)\n",
    "(np.sqrt(8 * np.pi * alpha) / (2 * np.pi * B)) * 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ".. _gallery:cs:ecg:bsbl:1:\n",
    "\n",
    "ECG Data Compressive Sensing\n",
    "=====================================\n",
    "\n",
    ".. contents::\n",
    "    :depth: 2\n",
    "    :local:\n",
    "\n",
    "In this example, we demonstrate the compressive sensing of ECG data\n",
    "and reconstruction using Block Sparse Bayesian Learning (BSBL).\n",
    "\"\"\"\n",
    "\n",
    "# Configure JAX to work with 64-bit floating point precision. \n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "# %% \n",
    "# Let's import necessary libraries\n",
    "import timeit\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "# CR-Suite libraries\n",
    "import cr.nimble as crn\n",
    "import cr.nimble.dsp as crdsp\n",
    "import cr.sparse.dict as crdict\n",
    "import cr.sparse.plots as crplot\n",
    "import cr.sparse.block.bsbl as bsbl\n",
    "\n",
    "# Sample data\n",
    "# from scipy.misc import electrocardiogram\n",
    "import mne\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# Miscellaneous\n",
    "from scipy.signal import detrend, butter, filtfilt\n",
    "\n",
    "\n",
    "# # %% \n",
    "# # Test signal\n",
    "# # ------------------------------\n",
    "# # SciPy includes a test electrocardiogram signal\n",
    "# # which is a 5 minute long electrocardiogram (ECG), \n",
    "# # a medical recording of the electrical activity of the heart, \n",
    "# # sampled at 360 Hz.\n",
    "# ecg = electrocardiogram()\n",
    "# # Sampling frequency in Hz\n",
    "# fs = 360\n",
    "# # We shall only process a part of the signal in this demo\n",
    "# N = 400\n",
    "# x = ecg[:N]\n",
    "# t = np.arange(N) * (1/fs)\n",
    "\n",
    "# Arquivo de exemplo da base \"CHB-MIT Scalp EEG Database\" \n",
    "raw = mne.io.read_raw_edf('./../files/chb01_14.edf')\n",
    "fs = raw.info['sfreq']\n",
    "\n",
    "# Captura de uma janela de 1s de um canal específico\n",
    "ch_name = raw.ch_names[4]\n",
    "start_time = 170\n",
    "end_time = 171\n",
    "\n",
    "x = raw.get_data(tmin=start_time, tmax=end_time, picks=(ch_name)).flatten()\n",
    "N = x.size\n",
    "t = np.linspace(start_time, end_time, num=N)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,4))\n",
    "ax.plot(t, x);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %% \n",
    "# Preprocessing\n",
    "# '''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "# Remove the linear trend from the signal\n",
    "x = detrend(x)\n",
    "## bandpass filter\n",
    "# lower cutoff frequency\n",
    "f1 = 5\n",
    "# upper cutoff frequency\n",
    "f2 = 40\n",
    "# passband in normalized frequency\n",
    "Wn = np.array([f1, f2]) * 2 / fs\n",
    "# butterworth filter\n",
    "fn = 3\n",
    "fb, fa = butter(fn, Wn, 'bandpass')\n",
    "x = filtfilt(fb,fa,x)\n",
    "fig, ax = plt.subplots(figsize=(16,4))\n",
    "ax.plot(t, x);\n",
    "\n",
    "\n",
    "\n",
    "# %% \n",
    "# Compressive Sensing at 70%\n",
    "# ------------------------------\n",
    "# We choose the compression ratio (M/N) to be 0.7\n",
    "CR = 0.70\n",
    "M = int(N * CR)\n",
    "print(f'M={M}, N={N}, CR={CR}')\n",
    "\n",
    "# %%\n",
    "# Sensing matrix\n",
    "Phi = crdict.gaussian_mtx(crn.KEY0, M, N)\n",
    "\n",
    "# %%\n",
    "# Measurements\n",
    "# '''''''''''''''''''''''''''''''''''''''\n",
    "y = Phi @ x\n",
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "ax.plot(y);\n",
    "\n",
    "\n",
    "# %% \n",
    "# Sparse Recovery with BSBL\n",
    "# '''''''''''''''''''''''''''''''''''''''\n",
    "options = bsbl.bsbl_bo_options(y, max_iters=20)\n",
    "start = timeit.default_timer()\n",
    "sol = bsbl.bsbl_bo_np_jit(Phi, y, 8, options=options)\n",
    "stop = timeit.default_timer()\n",
    "print(f'Reconstruction time: {stop - start:.2f} sec', )\n",
    "print(sol)\n",
    "\n",
    "# %%\n",
    "# Recovered signal\n",
    "x_hat = sol.x\n",
    "print(f'SNR: {crn.signal_noise_ratio(x, x_hat):.2f} dB, PRD: {crn.percent_rms_diff(x, x_hat):.1f}%')\n",
    "\n",
    "# %%\n",
    "# Plot the original and recovered signals\n",
    "ax = crplot.h_plots(2)\n",
    "ax[0].plot(x)\n",
    "ax[1].plot(x_hat)\n",
    "\n",
    "\n",
    "# %% \n",
    "# Compressive Sensing at 50%\n",
    "# ------------------------------\n",
    "# Let us now increase the compression\n",
    "CR = 0.50\n",
    "M = int(N * CR)\n",
    "print(f'M={M}, N={N}, CR={CR}')\n",
    "\n",
    "# %%\n",
    "# Sensing matrix\n",
    "Phi = crdict.gaussian_mtx(crn.KEY0, M, N)\n",
    "\n",
    "# %%\n",
    "# Measurements\n",
    "# '''''''''''''''''''''''''''''\n",
    "y = Phi @ x\n",
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "ax.plot(y);\n",
    "\n",
    "\n",
    "# %% \n",
    "# Sparse Recovery with BSBL\n",
    "# '''''''''''''''''''''''''''''''''''''''\n",
    "options = bsbl.bsbl_bo_options(y, max_iters=20)\n",
    "start = timeit.default_timer()\n",
    "sol = bsbl.bsbl_bo_np_jit(Phi, y, 8, options=options)\n",
    "stop = timeit.default_timer()\n",
    "print(f'Reconstruction time: {stop - start:.2f} sec', )\n",
    "print(sol)\n",
    "\n",
    "# %%\n",
    "# Recovered signal\n",
    "x_hat = sol.x\n",
    "print(f'SNR: {crn.signal_noise_ratio(x, x_hat):.2f} dB, PRD: {crn.percent_rms_diff(x, x_hat):.1f}%')\n",
    "\n",
    "# %%\n",
    "# Plot the original and recovered signals\n",
    "ax = crplot.h_plots(2)\n",
    "ax[0].plot(x)\n",
    "ax[1].plot(x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ea612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.fftpack import dct, idct\n",
    "import cvxpy as cp\n",
    "import cr.nimble as crn\n",
    "import cr.nimble.dsp as crdsp\n",
    "import cr.sparse.dict as crdict\n",
    "import cr.sparse.plots as crplot\n",
    "import cr.sparse.block.bsbl as bsbl\n",
    "\n",
    "class SamplingMatrix(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for sensing matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    _value: np.ndarray\n",
    "    \n",
    "    @property\n",
    "    def value(self) -> np.ndarray:\n",
    "        return self._value\n",
    "\n",
    "class SparseBasis(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for sparse basis.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def apply_sampling_matrix(self, Theta: np.ndarray) -> np.ndarray:\n",
    "        ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, s: np.ndarray) -> np.ndarray:\n",
    "        ...\n",
    "\n",
    "class ReconstructionAlgorithm(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for reconstruction algorithms.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reconstruct(self, y: np.ndarray, Phi: np.ndarray) -> np.ndarray:\n",
    "        ...\n",
    "\n",
    "class CompressiveSensing:\n",
    "    \"\"\"\n",
    "    Compressive Sensing framework.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sampling_matrix: SamplingMatrix, sparse_basis: SparseBasis, reconstruction_algorithm: ReconstructionAlgorithm):\n",
    "        self.sampling_matrix = sampling_matrix\n",
    "        self.sparse_basis = sparse_basis\n",
    "        self.reconstruction_algorithm = reconstruction_algorithm\n",
    "    \n",
    "    def solve(self, x: np.ndarray) -> np.ndarray:\n",
    "        y = self.sampling_matrix.value @ x\n",
    "        Phi = self.sparse_basis.apply_sampling_matrix(self.sampling_matrix.value)\n",
    "\n",
    "        s = self.reconstruction_algorithm.reconstruct(y, Phi)\n",
    "        return self.sparse_basis.backward(s)\n",
    "\n",
    "    def calc_metrics(self, x: np.ndarray, x_hat: np.ndarray) -> tuple[float, float] | tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculate metrics for the reconstruction.\n",
    "        \"\"\"\n",
    "\n",
    "        nmse = self._nmse(x, x_hat)\n",
    "        snr = self._snr(x, x_hat)\n",
    "        \n",
    "        return nmse, snr\n",
    "    \n",
    "    def _nmse(self, x: np.ndarray, x_hat: np.ndarray) -> float | np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate normalized mean square error (NMSE).\n",
    "\n",
    "        ||x - x_hat||^2 / ||x - mean(x)||^2\n",
    "        \"\"\"\n",
    "        mu = x.mean(axis=0)\n",
    "\n",
    "        den = np.linalg.norm(x - mu, axis=0)**2\n",
    "        num = np.linalg.norm(x - x_hat, axis=0)**2\n",
    "\n",
    "        return np.where(den == 0, np.inf, num / den)\n",
    "    \n",
    "    def _snr(self, x: np.ndarray, x_hat: np.ndarray) -> float | np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate signal to noise ratio (SNR).\n",
    "        \"\"\"\n",
    "        den = np.linalg.norm(x - x_hat, axis=0)**2\n",
    "        num = np.linalg.norm(x, axis=0)**2\n",
    "\n",
    "        return np.where(den == 0, np.inf, 10*np.log10(num / den))\n",
    "    \n",
    "\n",
    "class GaussianSamplingMatrix(SamplingMatrix):\n",
    "    \"\"\"\n",
    "    Gaussian sensing matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m: int, n: int, random_state: int = 42):\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        self._value = rng.standard_normal((m, n)) / np.sqrt(m)\n",
    "\n",
    "class Undersampled(SamplingMatrix):\n",
    "    \"\"\"\n",
    "    Undersampled identity matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m: int, n: int, random_state: int = 42):\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        indices = np.sort(rng.choice(n, m, replace=False))\n",
    "        self._value = np.eye(n)[indices, :]\n",
    "\n",
    "\n",
    "class BinaryPermutedBlockDiagonal(SamplingMatrix):\n",
    "    \"\"\"\n",
    "    Binary permuted block diagonal matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m: int, cr: int):\n",
    "        I = np.eye(m, dtype=int)\n",
    "        self._value = np.repeat(I, repeats=cr, axis=1)\n",
    "\n",
    "class SparseBinary(SamplingMatrix):\n",
    "    \"\"\"\n",
    "    Sparse binary matrix with `d` ones in each column in random rows.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m: int, n: int, d: int, random_state: int = 42):\n",
    "        if d > m:\n",
    "            raise ValueError(\"The number of ones per column (d) cannot exceed the number of rows (m).\")\n",
    "        \n",
    "        rng = np.random.default_rng(random_state)\n",
    "        self._value = np.zeros((m, n), dtype=int)\n",
    "        \n",
    "        for col in range(n):\n",
    "            # Randomly select `d` unique rows for each column\n",
    "            row_indices = rng.choice(m, d, replace=False)\n",
    "            self._value[row_indices, col] = 1\n",
    "\n",
    "class DCTBasis(SparseBasis):\n",
    "    \"\"\"\n",
    "    Discrete Cosine Transform (DCT) basis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n: int):\n",
    "        self._Psi = np.eye(n)\n",
    "    \n",
    "    def apply_sampling_matrix(self, Theta: np.ndarray) -> np.ndarray:\n",
    "        return dct(Theta, norm='ortho')\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return dct(x, norm='ortho')\n",
    "    \n",
    "    def backward(self, s: np.ndarray) -> np.ndarray:\n",
    "        return idct(s, norm='ortho')\n",
    "\n",
    "class GaborBasis(SparseBasis):\n",
    "    \"\"\"\n",
    "    Gabor basis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        N: int,\n",
    "        fs: float,\n",
    "        tf: int = 2,\n",
    "        ff: int = 4,\n",
    "        scales: list[int] | None = None,\n",
    "    ):\n",
    "        \"\"\"Create an over‑complete *real* Gabor dictionary Dₜ₍ₜf f₍ff) matching Table II.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        N         : Signal length (samples) — 512 in the paper.\n",
    "        fs        : Sampling frequency (Hz) — 128 in the paper.\n",
    "        tf, ff    : Time‑factor and frequency‑factor (1, 2, 4, 8).\n",
    "        scales    : Sequence of *scale* values *s* (Gaussian σ in samples). If *None*, the\n",
    "                    canonical set {1, 2, 4, 8, 16, 32, 64} is used.\n",
    "        time_base : Empirical constant giving the time step at scale 1 & tf = 1 (samples).\n",
    "        freq_base : Frequency step at scale 1 & ff = 1 (Hz).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        D         : (N × P) NumPy array with *P* atoms (column‑wise), ℓ₂‑normalised.\n",
    "        meta      : List of triplets (*scale, n0, f0*) for each atom.\n",
    "        \"\"\"\n",
    "        if scales is None:\n",
    "            scales = [1, 2, 4, 8, 16, 32, 64]\n",
    "\n",
    "        B = 2.0\n",
    "        alpha = 0.5 * np.log(0.5*(B + 1/B))\n",
    "\n",
    "        n = np.arange(N)\n",
    "        atoms: list[np.ndarray] = []\n",
    "        for s in scales:\n",
    "            time_base = s * B * np.sqrt(2 * alpha / np.pi)\n",
    "            ts = 4 * time_base / tf\n",
    "            n0_vals = np.arange(0, N, ts)\n",
    "\n",
    "            freq_base = (np.sqrt(8.0 * np.pi * alpha) / (s * B)) * (fs / 2.0)\n",
    "            fs_step = freq_base / ff  # Eq. 10 → frequency step (Hz)\n",
    "            f0_vals = np.arange(fs_step, fs / 2.0, fs_step)  # avoid DC / Nyquist\n",
    "\n",
    "            for n0 in n0_vals:\n",
    "                for f0 in f0_vals:\n",
    "                    atom = self._gabor_atom(n, n0, f0, s)\n",
    "                    atoms.append(atom)\n",
    "\n",
    "        D = np.stack(atoms, axis=1)  # (N × P)\n",
    "        self._Psi = D.astype(np.float32)\n",
    "    \n",
    "    def apply_sampling_matrix(self, Theta: np.ndarray) -> np.ndarray:\n",
    "        return Theta @ self._Psi\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self._Psi @ x\n",
    "    \n",
    "    def backward(self, s: np.ndarray) -> np.ndarray:\n",
    "        return self._Psi @ s\n",
    "    \n",
    "    def _gabor_atom(self, n: list[int], n0: float, f0: float, s: float, phi: float = 0.0) -> np.ndarray:\n",
    "        \"\"\"Generate a *real* Gabor atom (eq. 8 in the paper).\n",
    "\n",
    "        G(x) = exp(-(x‑n0)² / (2 s²)) · cos(2π f0 (x‑n0)/fs + φ)\n",
    "        The atom is ℓ₂‑normalised.\n",
    "        \"\"\"\n",
    "        atom = np.exp(-(n - n0) ** 2 / (2.0 * s ** 2)) * np.sin(2.0 * np.pi * f0 * (n - n0) + phi)\n",
    "        atom /= np.linalg.norm(atom)\n",
    "        return atom.astype(np.float32)\n",
    "\n",
    "class BasisPursuit(ReconstructionAlgorithm):\n",
    "    \"\"\"\n",
    "    Basis Pursuit.\n",
    "    \"\"\"\n",
    "    \n",
    "    def reconstruct(self, y: np.ndarray, Phi: np.ndarray) -> np.ndarray:\n",
    "        alpha = cp.Variable(Phi.shape[1])\n",
    "        objective = cp.Minimize(cp.norm1(alpha))\n",
    "        constraints = [y == Phi @ alpha]\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        prob.solve()\n",
    "        if prob.status != cp.OPTIMAL:\n",
    "            raise ValueError(\"The optimization problem is not solvable.\")\n",
    "        \n",
    "        return np.asarray(alpha.value, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "class OrthogonalMatchingPursuit(ReconstructionAlgorithm):\n",
    "    \"\"\"\n",
    "    Orthogonal Matching Pursuit (OMP) reconstruction algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_iter: int = 100, tol: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_iter : int\n",
    "            Maximum number of iterations for the OMP algorithm.\n",
    "        tol : float\n",
    "            Tolerance for the residual norm to stop the algorithm.\n",
    "        \"\"\"\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def reconstruct(self, y: np.ndarray, Phi: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reconstruct the sparse signal using OMP.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : np.ndarray\n",
    "            The observed measurements (m-dimensional vector).\n",
    "        Phi : np.ndarray\n",
    "            The sensing matrix (m x n matrix).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The reconstructed sparse signal (n-dimensional vector).\n",
    "        \"\"\"\n",
    "        m, n = Phi.shape\n",
    "        residual = y.copy()\n",
    "        support = []\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            # Step 1: Find the column of Phi most correlated with the residual\n",
    "            correlations = Phi.T @ residual\n",
    "            best_index = np.argmax(np.abs(correlations))\n",
    "            support.append(best_index)\n",
    "\n",
    "            # Step 2: Solve least squares problem to update the solution\n",
    "            Phi_support = Phi[:, support]\n",
    "            s_support = np.linalg.lstsq(Phi_support, y, rcond=None)[0]\n",
    "\n",
    "            # Step 3: Update the residual\n",
    "            residual = y - Phi_support @ s_support\n",
    "\n",
    "            # Check stopping condition\n",
    "            if np.linalg.norm(residual) < self.tol:\n",
    "                break\n",
    "\n",
    "        # Step 4: Construct the full solution vector\n",
    "        s = np.zeros(n)\n",
    "        s[support] = s_support\n",
    "    \n",
    "        return s\n",
    "\n",
    "class SimultaneousOrthogonalMatchingPursuit(ReconstructionAlgorithm):\n",
    "    \"\"\"\n",
    "    Simultaneous Orthogonal Matching Pursuit (SOMP) reconstruction algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_iter: int = 100, tol: float = 1e-6):\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def reconstruct(self, Y: np.ndarray, Phi: np.ndarray):\n",
    "        \"\"\"\n",
    "        Simultaneous Orthogonal Matching Pursuit (SOMP)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y   : array, shape (m, K)\n",
    "            Matrix of K measurement vectors (each column is one measurement).\n",
    "        Phi : array, shape (m, n)\n",
    "            Dictionary matrix whose columns are the atoms.\n",
    "        s   : int\n",
    "            Sparsity level: number of atoms to select.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        S   : list of int\n",
    "            Indices of the selected atoms (size s).\n",
    "        X   : array, shape (s, K)\n",
    "            Coefficient matrix (only the rows corresponding to S are nonzero).\n",
    "        \"\"\"\n",
    "        # Initialize residual and support\n",
    "        R = Y.copy()                    # residual: shape (m, K)\n",
    "        S = []                          # support set (list of selected indices)\n",
    "        \n",
    "        # Precompute Phi^T for efficiency\n",
    "        PhiT = Phi.T                  # shape (n, m)\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            # 1) Compute (absolute) correlations for each atom j: sum over all K signals\n",
    "            #    corr[j] = || (Phi[:, j].T @ R) ||_1\n",
    "            # Using matrix form: (Phi^T @ R) is (n x K), then take abs and sum across columns.\n",
    "            corr = np.sum(np.abs(PhiT @ R), axis=1)  # shape (n,)\n",
    "            \n",
    "            # 2) Mask out already-selected atoms (set their score to -inf)\n",
    "            mask = np.ones_like(corr, dtype=bool)\n",
    "            mask[S] = False\n",
    "            corr_masked = np.where(mask, corr, -np.inf)\n",
    "            \n",
    "            # 3) Pick the atom with maximum correlation\n",
    "            j_best = int(np.argmax(corr_masked))\n",
    "            S.append(j_best)\n",
    "            \n",
    "            # 4) Form the sub-dictionary with selected atoms\n",
    "            Phi_S = Phi[:, S]           # shape (m, |S|)\n",
    "            \n",
    "            # 5) Solve least-squares to get coefficients for all K signals:\n",
    "            #    X_S = Phi_S^+ Y, where Phi_S^+ is the Moore–Penrose pseudoinverse\n",
    "            X_S = np.linalg.pinv(Phi_S) @ Y  # shape (|S|, K)\n",
    "            \n",
    "            # 6) Update residuals: R = Y - Phi_S X_S\n",
    "            R = Y - Phi_S @ X_S           # shape (m, K)\n",
    "\n",
    "            # 7) Check stopping condition\n",
    "            if np.linalg.norm(R) < self.tol:\n",
    "                break\n",
    "        \n",
    "        # 8) Construct the full solution vector\n",
    "        # X: full n×K coefficient matrix, but only rows in S are nonzero.\n",
    "        X = np.zeros((Phi.shape[1], Y.shape[1]))\n",
    "        X[S, :] = X_S\n",
    "        \n",
    "        return X\n",
    "\n",
    "def get_signals(start_time: int, end_time: int) -> tuple[np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Get a sample EEG signal from the CHB-MIT Scalp EEG Database.\n",
    "    \"\"\"\n",
    "    # Load the raw EEG data\n",
    "    raw = mne.io.read_raw_edf('./../files/chb01_14.edf', verbose=False)\n",
    "    fs = raw.info['sfreq']\n",
    "    \n",
    "    # Get the data for the selected channel and time window\n",
    "    X = raw.get_data(tmin=start_time, tmax=end_time).T\n",
    "    t = np.linspace(start_time, end_time, num=X.shape[0])\n",
    "    \n",
    "    return X, t, fs\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, t, fs = get_signals(\n",
    "        start_time=1,\n",
    "        end_time=2\n",
    "    )\n",
    "    n = X.shape[0]\n",
    "\n",
    "    cr = 2\n",
    "    m = int(n / cr)\n",
    "\n",
    "    # ----------------------------------------------- 1° Etapa: Definir a matriz de amostragem --------------------------------------------------\n",
    "    # sampling_matrix = SparseBinary(m, n, d = 8, random_state=42)\n",
    "    # sampling_matrix = GaussianSamplingMatrix(m, n)\n",
    "    # sampling_matrix = Undersampled(m, n)\n",
    "    sampling_matrix = BinaryPermutedBlockDiagonal(m, cr)\n",
    "\n",
    "    # ----------------------------------------------- 2° Etapa: Definir dicionário --------------------------------------------------------------\n",
    "    sparse_basis = DCTBasis(n)\n",
    "    # sparse_basis = GaborBasis(n, fs=fs, tf=2, ff=4)\n",
    "\n",
    "    # ----------------------------------------------- 3° Etapa: Definir algoritmo de reconstrução -----------------------------------------------\n",
    "    reconstruction_algorithm = SimultaneousOrthogonalMatchingPursuit(max_iter=250, tol=1e-8)\n",
    "    \n",
    "    # ----------------------------------------------- 4° Etapa: Aplicar compressed sensing ------------------------------------------------------\n",
    "    cs = CompressiveSensing(sampling_matrix, sparse_basis, reconstruction_algorithm)\n",
    "    X_hat = cs.solve(X)\n",
    "\n",
    "    # ----------------------------------------------- 5° Etapa: Avaliar resultado ---------------------------------------------------------------\n",
    "    # nmse, snr = cs.calc_metrics(X[:, 18], X_hat[:, 18])\n",
    "    # print(f\"NMSE: {nmse:.4f}, SNR: {snr:.4f} dB\")\n",
    "    nmse, snr = cs.calc_metrics(X, X_hat)\n",
    "    print(f\"nmse = {nmse}\", f\"snr = {snr}\")\n",
    "    # print(f\"NMSE: {nmse:.4f}, SNR: {snr:.4f} dB\")\n",
    "\n",
    "    # ----------------------------------------------- 6° Etapa: Plotar curvas -------------------------------------------------------------------\n",
    "    # fig, ax = plt.subplots(figsize=[15, 5])\n",
    "    # ax.plot(t, X, 'k', label='Sinal original')\n",
    "    # ax.plot(t, X_hat, 'r', linestyle='dotted', label='Sinal reconstruído')\n",
    "    # ax.set_xlabel('Tempo (s)')\n",
    "    # ax.set_ylabel('Amplitude')\n",
    "    # ax.set_title('Sinal original x Sinal reconstruído')\n",
    "    # ax.legend(loc='upper right', framealpha=1)\n",
    "    # ax.grid()\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "def main2():\n",
    "    X, t, fs = get_signals(\n",
    "        start_time=1,\n",
    "        end_time=2\n",
    "    )\n",
    "    \n",
    "    x = X[:, 0]\n",
    "    n = x.size\n",
    "\n",
    "    cr = 2\n",
    "    m = int(n / cr)\n",
    "\n",
    "    # ----------------------------------------------- 1° Etapa: Definir a matriz de amostragem --------------------------------------------------\n",
    "    # sampling_matrix = SparseBinary(m, n, d = 8, random_state=42)\n",
    "    # sampling_matrix = GaussianSamplingMatrix(m, n)\n",
    "    # sampling_matrix = Undersampled(m, n)\n",
    "    sampling_matrix = BinaryPermutedBlockDiagonal(m, cr)\n",
    "\n",
    "    # ----------------------------------------------- 2° Etapa: Definir dicionário --------------------------------------------------------------\n",
    "    sparse_basis = DCTBasis(n)\n",
    "    # sparse_basis = GaborBasis(n, fs=fs, tf=2, ff=4)\n",
    "\n",
    "    # ----------------------------------------------- 3° Etapa: Definir algoritmo de reconstrução -----------------------------------------------\n",
    "    reconstruction_algorithm = BasisPursuit()\n",
    "    # reconstruction_algorithm = OrthogonalMatchingPursuit(max_iter=250, tol=1e-8)\n",
    "\n",
    "    # ----------------------------------------------- 4° Etapa: Aplicar compressed sensing ------------------------------------------------------\n",
    "    cs = CompressiveSensing(sampling_matrix, sparse_basis, reconstruction_algorithm)\n",
    "    x_hat = cs.solve(x)\n",
    "\n",
    "    # ----------------------------------------------- 5° Etapa: Avaliar resultado ---------------------------------------------------------------\n",
    "    nmse, snr = cs.calc_metrics(x, x_hat)\n",
    "    print(f\"NMSE: {nmse:.4f}, SNR: {snr:.4f} dB\")\n",
    "\n",
    "    # ----------------------------------------------- 6° Etapa: Plotar curvas -------------------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=[15, 5])\n",
    "    ax.plot(t, x, 'k', label='Sinal original')\n",
    "    ax.plot(t, x_hat, 'r', linestyle='dotted', label='Sinal reconstruído')\n",
    "    ax.set_xlabel('Tempo (s)')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.set_title('Sinal original x Sinal reconstruído')\n",
    "    ax.legend(loc='upper right', framealpha=1)\n",
    "    ax.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main()\n",
    "    main2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db43e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as mt\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\" (Robust) Compressed Sensing (CS) and harmonic signals (sum of sinusoids)\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    S : int\n",
    "        Number of non-zeros coefficients of the S-sparse signal representation.\n",
    "    pts : int\n",
    "        Number of points of the original signal (sinusoid sum).\n",
    "    n : int\n",
    "        Number of FFT points and, consequently, of the sparse signal points.\n",
    "    N : int\n",
    "        Number of columns of the sensing matrix.\n",
    "    M : int\n",
    "        Number of rows of the sensing matrix.\n",
    "    t : (pts,) ndarray\n",
    "        Range (in rads) of values used by the reference sinusoid.\n",
    "    f : (S,) ndarray\n",
    "        Random frequency values with uniform distribution.\n",
    "    A : (S,) list\n",
    "        Random amplitude values with uniform distribution.\n",
    "    Phi : (M,N) ndarray\n",
    "        Sensing (aquisition) matrix generated from i.i.d Gaussian samples.\n",
    "    wNoise : (M,) ndarray\n",
    "        Sensing noise with (Normal) Gaussian distribution ~ N(0,1).\n",
    "    s : (pts,) ndarray\n",
    "        Superposition of S sinusoids with random frequencies/amplitudes.\n",
    "    fft : (n,) ndarray\n",
    "        DFT of the sum of sinusoids using FFT algorithm.\n",
    "    x : (n,) ndarray\n",
    "        S-sparse (compressible) signal.\n",
    "    idx : (n,) ndarray\n",
    "        Sorted index values of largest coefficients (in magnitude) in 'x'.\n",
    "    y : (M,) ndarray\n",
    "        Compressed signal.\n",
    "    eps : float\n",
    "        Second Order Cone Program (SOCP) constraint constant for robust CS.\n",
    "    xOpt\n",
    "        CVXPY optimization problem variable and sparsest solution.\n",
    "    SOC_constraints : list\n",
    "        CVXPY problem second order cone constraints for robust CS.\n",
    "    prob\n",
    "        CVXPY optimization probem formulation.\n",
    "    sCs : (pts,) ndarray\n",
    "        Signal (sum of sinusoids) reconstruction with CS.\n",
    "    fs : int\n",
    "        Sampling frequency for linear (Nyquist-Shannon) reconstruction.\n",
    "    Psi : (pts,pts) ndarray\n",
    "        Sensing (aquisition) identity matrix generated for linear sampling.\n",
    "    uniIdx : (fs,) ndarray\n",
    "        Indexes of the positions sampled from the original signal (Dirac comb).\n",
    "    yNs : (fs,) ndarray\n",
    "        Signal after linear sampling.\n",
    "    sNs : (pts,) ndarray\n",
    "        Signal (sum of sinusoids) reconstruction with sinc interpolation.\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This code implements a (robust) CS scheme in which a sum of sinusoids with\n",
    "    random amplitudes/frequencies is recovered using l_1 minimization. For\n",
    "    comparison purposes, this simulation also includes the Nyquist-Shannon\n",
    "    linear sampling scheme.\n",
    "\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] E. J. Candes and M. B. Wakin, \"An Introduction To Compressive\n",
    "           Sampling,\" in IEEE Signal Processing Magazine, vol. 25, no. 2,\n",
    "           pp. 21-30, March 2008.\n",
    "\n",
    "    .. [2] Han, Z., Li, H., & Yin, W. (2013). Compressive Sensing for Wireless\n",
    "           Networks. Cambridge: Cambridge University.\n",
    "\n",
    "    .. [3] Eldar, Y., & Kutyniok, G. (Eds.). (2012). Compressed Sensing:\n",
    "           Theory and Applications. Cambridge: Cambridge University Press.\n",
    "\n",
    "    .. [4] S. Boyd and L. Vandenberghe, Convex Optimization. New York, NY, USA:\n",
    "           Cambridge University Press, 2004.\n",
    "\n",
    "\n",
    "    © 2020 Pedro H. C. de Souza\n",
    "\"\"\"\n",
    "S = 16\n",
    "pts = 2047\n",
    "n = (pts + 1)//2\n",
    "N = n  # the 'n'-point FFT sparsifies the signal, thus dim(Range('Phi')) = 'n'\n",
    "M = 91  # m >= C*S*log(N/S) for i.i.d Gaussian sensing matrix (to obey RIP)\n",
    "t = np.linspace(0, 2*np.pi, pts)  # ref. sinusoid period spans 'pts' samples\n",
    "np.random.seed(1)  # repeat a run with the same aplitudes/frequecies values\n",
    "f = np.random.randint(1, 100, S)  # location of the S non-zero coefficients\n",
    "A = np.random.randint(5, 10, S)  # magnitude of the S non-zero coefficients\n",
    "np.random.seed()  # a new realization of the sensing matrix for each run\n",
    "Phi = np.random.randn(M, N)/mt.sqrt(M)  # ~ N(0,1/M) (variance is normalized)\n",
    "wNoise = (np.random.randn(M) + 1j*np.random.randn(M))/mt.sqrt(2)  # ~ N(0,1)\n",
    "\n",
    "s = np.zeros(pts)\n",
    "for i in range(S):\n",
    "    s += A[i]*np.sin(f[i]*t)  # generate the sum of sinusoids\n",
    "fft = np.fft.rfft(s, norm=\"ortho\")\n",
    "# x = fft  # uncomment (comment below) to add signal noise\n",
    "\n",
    "# Optimum thresholding strategy (see [3, pg.11])\n",
    "x = np.zeros(N) + 1j*np.zeros(N)\n",
    "idx = np.argsort(-np.abs(fft)**2)  # sort coefficients in descend order\n",
    "x[idx[0:S]] = fft[idx[0:S]]  # oracle that selects the largest coefficients\n",
    "\n",
    "# Universal encoding\n",
    "y = Phi@x.T  # sensing (aquisition) of the sparse (compressible) signal\n",
    "# y = Phi@x.T + wNoise  # uncomment (comment above) to add sensing noise\n",
    "\n",
    "# l_1 minimization reconstruction\n",
    "xOpt = cp.Variable(N, complex=True)\n",
    "# eps = np.linalg.norm(wNoise)  # see [2, pg.64] Theorem 4\n",
    "# SOC_constraints = [cp.SOC(cp.real(eps), Phi@xOpt.T - y)]  # uncomment\n",
    "# prob = cp.Problem(cp.Minimize(cp.norm(xOpt, p=1)),        # (comment below)\n",
    "#                   SOC_constraints)                        # for robust CS\n",
    "prob = cp.Problem(cp.Minimize(cp.norm(xOpt, p=1)),\n",
    "                  [Phi@xOpt.T == y])  # l_1 minization problem in CVXPY\n",
    "prob.solve()\n",
    "sCs = np.fft.irfft(xOpt.value, pts, norm=\"ortho\")  # IFFT performed on \"x*\"\n",
    "\n",
    "# Nyquist-Shannon linear sampling\n",
    "fs = 161\n",
    "Psi = np.identity(pts)\n",
    "uniIdx = np.round(np.arange(0, pts - 1, pts/fs)).astype(int)\n",
    "yNs = Psi[uniIdx, :]@s.T\n",
    "sNs = np.zeros(pts)\n",
    "for i in range(pts):\n",
    "    sNs[i] = yNs @ np.sinc((t[i]/(2*np.pi) - np.arange(fs)*(1/fs))/(1/fs))\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots()\n",
    "plt.subplot(211)\n",
    "line1, = plt.plot(t, s, 'k', label='Original')\n",
    "line2, = plt.plot(t, sNs, '--g', label='Nyquist-Shannon')\n",
    "line3, = plt.plot(t, sCs, ':r', label='CS')\n",
    "plt.axis([np.pi, np.pi + 0.5, np.min(s), np.max(s)])\n",
    "plt.xlabel('samples')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xticks(visible=False)\n",
    "plt.yticks(visible=False)\n",
    "plt.legend(fontsize='small')\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "textstr = '\\n'.join((\n",
    "    'Nyquist-Shannon \\n M = %.2i samples \\n $|| \\mathbf{s^*} - \\mathbf{s^o} ||^2_2 /N$ = %.2E' %\n",
    "    (fs, np.linalg.norm(sNs - s)**2 / s.size / (sum(A**2) / 2)),\n",
    "    'CS \\n M = %.2i samples \\n $|| \\mathbf{s^*} - \\mathbf{s^o} ||^2_2 /N$ = %.2E' %\n",
    "    (M, np.linalg.norm(sCs - s)**2 / s.size / (sum(A**2) / 2))))\n",
    "plt.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize='xx-small',\n",
    "         verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.subplot(212)\n",
    "normCf = (np.linalg.norm(s)**2)/2\n",
    "line1, = plt.plot(np.arange(n), (np.abs(x)**2)/normCf,\n",
    "                  'xb', markersize=4, label='Original (FFT)')\n",
    "line2, = plt.plot(np.arange(n), (np.abs(xOpt.value)**2)/normCf, 'or', fillstyle='none',\n",
    "                  markersize=4, label='CS')\n",
    "plt.axis([0, np.max(f) + 20, 0, 0.15])\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.yticks(visible=False)\n",
    "plt.legend(fontsize='small')\n",
    "textstr = r'$S = %i$' % (S, )\n",
    "plt.text(0.9, 0.05, textstr, transform=ax.transAxes, fontsize='small',\n",
    "         verticalalignment='bottom', bbox=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da046b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import solve # For solving linear systems\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def frob_norm_sq(X):\n",
    "    \"\"\"Calculates the squared Frobenius norm of a matrix X.\"\"\"\n",
    "    return np.linalg.norm(X, 'fro')**2\n",
    "\n",
    "def l21_norm(X):\n",
    "    \"\"\"Calculates the l2,1-norm of a matrix X (sum of L2 norms of rows).\"\"\"\n",
    "    # Calculate L2 norm for each row\n",
    "    row_l2_norms = np.linalg.norm(X, axis=1)\n",
    "    # Sum the L2 norms\n",
    "    return np.sum(row_l2_norms)\n",
    "\n",
    "def soft_threshold_row_sparse(X, lambda_val, mu_val):\n",
    "    \"\"\"\n",
    "    Implements the row-sparse soft-thresholding operation for updating Q.\n",
    "    Based on Equation (12d) and the definition of Lambda in the paper.\n",
    "\n",
    "    Q <- signum(X) * max(0, |X| - (lambda/mu) * Lambda)\n",
    "    where Lambda = diag(||X_j||_2^-1) * abs(X)\n",
    "    \"\"\"\n",
    "    # X corresponds to (Z + B2) in the paper's notation\n",
    "    \n",
    "    # Calculate L2 norm for each row of X\n",
    "    row_l2_norms = np.linalg.norm(X, axis=1, keepdims=True) # Keepdims for broadcasting\n",
    "    \n",
    "    # Handle rows with zero L2 norm to avoid division by zero.\n",
    "    # If a row's L2 norm is zero, its inverse is effectively zero for scaling.\n",
    "    # This also ensures that if a row is all zeros, it remains all zeros after thresholding.\n",
    "    inv_row_l2_norms = np.zeros_like(row_l2_norms)\n",
    "    non_zero_rows_idx = row_l2_norms.flatten() != 0\n",
    "    inv_row_l2_norms[non_zero_rows_idx] = 1.0 / row_l2_norms[non_zero_rows_idx]\n",
    "\n",
    "    # Calculate Lambda matrix as defined in the paper\n",
    "    # Lambda_jk = (1 / ||X_j||_2) * |X_jk|\n",
    "    Lambda_matrix = inv_row_l2_norms * np.abs(X)\n",
    "\n",
    "    # Calculate the threshold term\n",
    "    threshold_term = (lambda_val / mu_val) * Lambda_matrix\n",
    "\n",
    "    # Apply the soft-thresholding\n",
    "    # |X| - threshold_term\n",
    "    arg_max = np.abs(X) - threshold_term\n",
    "    \n",
    "    # max(0, arg_max)\n",
    "    max_val = np.maximum(0, arg_max)\n",
    "    \n",
    "    # signum(X) * max_val\n",
    "    Q_new = np.sign(X) * max_val\n",
    "    \n",
    "    return Q_new\n",
    "\n",
    "def calculate_objective_function(Y, A, D, Z, P, Q, B1, B2, lambda1, lambda2, mu1, mu2):\n",
    "    \"\"\"\n",
    "    Calculates the Augmented Lagrangian objective function (Equation 11).\n",
    "    \"\"\"\n",
    "    term1 = frob_norm_sq(Y - A @ D @ Z)\n",
    "    term2 = lambda1 * frob_norm_sq(P)\n",
    "    term3 = lambda2 * l21_norm(Q) # Using lambda2 for l21_norm as per (9) and (12d)\n",
    "    term4 = mu1 * frob_norm_sq(P - D - B1)\n",
    "    term5 = mu2 * frob_norm_sq(Q - Z - B2)\n",
    "    \n",
    "    return term1 + term2 + term3 + term4 + term5\n",
    "\n",
    "# --- Main Algorithm Implementation ---\n",
    "\n",
    "def row_sparse_bcs(Y, A, lambda1, lambda2, mu1, mu2,\n",
    "                   n_features_dict, max_iterations=100, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    Implements the Row-Sparse Blind Compressed Sensing (BCS) algorithm\n",
    "    using Split Bregman as described in Shukla & Majumdar (2015).\n",
    "\n",
    "    Args:\n",
    "        Y (np.ndarray): Compressed multi-channel EEG signal (m x num_channels).\n",
    "        A (np.ndarray): Random projection matrix (m x n).\n",
    "        lambda1 (float): Regularization parameter for D (Frobenius norm).\n",
    "        lambda2 (float): Regularization parameter for Z (l2,1-norm).\n",
    "        mu1 (float): Bregman parameter for P-D constraint.\n",
    "        mu2 (float): Bregman parameter for Q-Z constraint.\n",
    "        n_features_dict (int): Number of atoms in the dictionary D.\n",
    "        max_iterations (int): Maximum number of outer iterations.\n",
    "        tolerance (float): Tolerance for objective function change to stop.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (D_hat, Z_hat, P_hat, Q_hat) - estimated matrices.\n",
    "    \"\"\"\n",
    "    \n",
    "    m, num_channels = Y.shape\n",
    "    n = A.shape[1] # Original signal dimension\n",
    "\n",
    "    # Initialize D, Z, P, Q, B1, B2\n",
    "    # D: (n x n_features_dict)\n",
    "    # Z: (n_features_dict x num_channels)\n",
    "    # P: (n x n_features_dict) - proxy for D\n",
    "    # Q: (n_features_dict x num_channels) - proxy for Z\n",
    "    # B1: (n x n_features_dict) - Bregman variable for P-D\n",
    "    # B2: (n_features_dict x num_channels) - Bregman variable for Q-Z\n",
    "\n",
    "    # Paper suggests D initialized to random values, number of columns > 30 (e.g., 40)\n",
    "    D = np.random.randn(n, n_features_dict)\n",
    "    \n",
    "    # Paper suggests B1, B2 initialized to all ones.\n",
    "    B1 = np.ones((n, n_features_dict))\n",
    "    B2 = np.ones((n_features_dict, num_channels))\n",
    "\n",
    "    # Initialize Z and P. Often zeros or random small values.\n",
    "    # For P, it's a proxy for D, so it should have same shape as D.\n",
    "    P = np.zeros((n, n_features_dict))\n",
    "    # For Q, it's a proxy for Z, so it should have same shape as Z.\n",
    "    Q = np.zeros((n_features_dict, num_channels))\n",
    "    # Z is the sparse coefficient matrix, so it should have shape (n_features_dict, num_channels).\n",
    "    Z = np.zeros((n_features_dict, num_channels))\n",
    "\n",
    "\n",
    "    prev_objective_value = float('inf')\n",
    "\n",
    "    print(\"Starting Row-Sparse BCS Algorithm...\")\n",
    "    print(f\"Initial objective value (approx): {calculate_objective_function(Y, A, D, Z, P, Q, B1, B2, lambda1, lambda2, mu1, mu2):.4f}\")\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        # --- 1. Solve for Z (Sub-problem 12b) ---\n",
    "        # min_Z ||Y - ADZ||_F^2 + mu2 ||Q - Z - B2||_F^2\n",
    "        # Derivative w.r.t Z set to zero:\n",
    "        # (D^T A^T A D + mu2 * I) Z = D^T A^T Y + mu2 * (Q - B2)\n",
    "        \n",
    "        Left_Z = D.T @ A.T @ A @ D + mu2 * np.eye(n_features_dict)\n",
    "        Right_Z = D.T @ A.T @ Y + mu2 * (Q - B2)\n",
    "        \n",
    "        # Solve the linear system for Z\n",
    "        Z = solve(Left_Z, Right_Z)\n",
    "\n",
    "        # --- 2. Solve for D (Sub-problem 12a) ---\n",
    "        # min_D ||Y - ADZ||_F^2 + mu1 ||P - D - B1||_F^2\n",
    "        # Derivative w.r.t D set to zero:\n",
    "        # A^T A D Z Z^T + mu1 D = A^T Y Z^T + mu1 * (P - B1)\n",
    "        # This is a generalized Sylvester equation of the form M D N + Q D = S.\n",
    "        # Solved by vectorizing D and forming a large linear system.\n",
    "        \n",
    "        # Coefficients for vec(D)\n",
    "        C1 = A.T @ A # (n x n)\n",
    "        C2 = Z @ Z.T # (n_features_dict x n_features_dict)\n",
    "\n",
    "        # Matrix for vec(D) in the linear system (K_D @ vec(D) = RHS_D_flat)\n",
    "        # K_D = kron(C2.T, C1) + mu1 * I_vec(D)\n",
    "        # The Kronecker product order is important: kron(B, A) for AXB form.\n",
    "        # Here it's A D Z, so the matrix for vec(D) is (Z.T @ Z) kron (A.T @ A)\n",
    "        # Let's verify the Kronecker product for M D N + Q D = S.\n",
    "        # Vectorizing: (N.T kron M + kron(I, Q)) vec(D) = vec(S)\n",
    "        # In our case: M=A.T@A, N=Z@Z.T, Q=mu1*I, S=A.T@Y@Z.T + mu1*(P-B1)\n",
    "        \n",
    "        K_D = np.kron(C2.T, C1) + mu1 * np.eye(D.size) # D.size = n * n_features_dict\n",
    "        \n",
    "        # RHS of the linear system, flattened\n",
    "        RHS_D = A.T @ Y @ Z.T + mu1 * (P - B1)\n",
    "        RHS_D_flat = RHS_D.flatten() # Flatten to a 1D array\n",
    "        \n",
    "        # Solve for the vectorized D and reshape back\n",
    "        vec_D = solve(K_D, RHS_D_flat)\n",
    "        D = vec_D.reshape(D.shape)\n",
    "\n",
    "        # --- 3. Solve for P (Sub-problem 12c) ---\n",
    "        # min_P lambda1 ||P||_F^2 + mu1 ||P - D - B1||_F^2\n",
    "        # Closed-form solution: P = (mu1 / (lambda1 + mu1)) * (D + B1)\n",
    "        \n",
    "        P = (mu1 / (lambda1 + mu1)) * (D + B1)\n",
    "\n",
    "        # --- 4. Update Q (Sub-problem 12d) ---\n",
    "        # Q <- signum(Z+B2) * max(0, |Z+B2| - (lambda2/mu2) * Lambda)\n",
    "        # Lambda = diag(||(Z+B2)_j||_2^-1) * abs(Z+B2)\n",
    "        \n",
    "        Q = soft_threshold_row_sparse(Z + B2, lambda2, mu2)\n",
    "\n",
    "        # --- 5. Update Bregman Variables ---\n",
    "        # As per the paper's explicit formulas:\n",
    "        B1 = P - D - B1\n",
    "        B2 = Q - Z - B2\n",
    "        \n",
    "        # --- Check for Convergence ---\n",
    "        current_objective_value = calculate_objective_function(Y, A, D, Z, P, Q, B1, B2, lambda1, lambda2, mu1, mu2)\n",
    "        \n",
    "        change_in_objective = abs(prev_objective_value - current_objective_value)\n",
    "        \n",
    "        print(f\"Iteration {iteration+1}: Objective = {current_objective_value:.4f}, Change = {change_in_objective:.6f}\")\n",
    "        \n",
    "        if change_in_objective < tolerance:\n",
    "            print(f\"Converged at iteration {iteration+1}.\")\n",
    "            break\n",
    "            \n",
    "        prev_objective_value = current_objective_value\n",
    "    else:\n",
    "        print(f\"Maximum iterations ({max_iterations}) reached.\")\n",
    "\n",
    "    return D, Z, P, Q\n",
    "\n",
    "# --- Synthetic Data Generation ---\n",
    "# Dimensions (adjust as needed for your experiments)\n",
    "m = 128  # Number of measurements (rows of Y, A)\n",
    "n = 256  # Original signal dimension (cols of A, rows of X=DZ)\n",
    "num_channels = 4 # Number of EEG channels (cols of Y, Z)\n",
    "n_features_dict = 40 # Number of atoms in dictionary D (cols of D, rows of Z) - as per paper\n",
    "\n",
    "# Generate random projection matrix A (e.g., Bernoulli or Gaussian)\n",
    "# Using Bernoulli matrix as it's common in Compressed Sensing\n",
    "A = np.random.choice([-1, 1], size=(m, n)) / np.sqrt(m)\n",
    "\n",
    "# Generate true dictionary D_true (n x n_features_dict)\n",
    "D_true = np.random.randn(n, n_features_dict)\n",
    "\n",
    "# Generate true sparse coefficients Z_true (n_features_dict x num_channels)\n",
    "# Make it row-sparse by setting some rows to zero\n",
    "Z_true = np.random.randn(n_features_dict, num_channels)\n",
    "sparsity_ratio = 0.5 # Percentage of rows to keep non-zero\n",
    "num_sparse_rows = int(n_features_dict * sparsity_ratio)\n",
    "zero_rows_indices = np.random.choice(n_features_dict, n_features_dict - num_sparse_rows, replace=False)\n",
    "Z_true[zero_rows_indices, :] = 0\n",
    "\n",
    "# Generate Y_true = A @ D_true @ Z_true\n",
    "Y_true = A @ D_true @ Z_true\n",
    "\n",
    "# Add Gaussian noise to Y\n",
    "noise_level = 0.05 # Adjust noise level as desired\n",
    "noise = noise_level * np.random.randn(*Y_true.shape)\n",
    "Y = Y_true + noise\n",
    "\n",
    "# --- Algorithm Parameters (from the paper) ---\n",
    "lambda1 = 0.01 # Example value, paper uses lambda1 for D, but doesn't specify a value.\n",
    "               # I'll use a small value to encourage D to be somewhat structured.\n",
    "lambda2 = 0.01 # Example value, paper uses lambda2 for Z, but doesn't specify a value.\n",
    "               # This controls the sparsity of Z.\n",
    "mu1 = 0.1      # Bregman parameter for P-D constraint (from paper)\n",
    "mu2 = 0.001    # Bregman parameter for Q-Z constraint (from paper)\n",
    "\n",
    "# --- Run the Algorithm ---\n",
    "D_hat, Z_hat, P_hat, Q_hat = row_sparse_bcs(\n",
    "    Y, A, lambda1, lambda2, mu1, mu2, n_features_dict\n",
    ")\n",
    "\n",
    "# --- Evaluate Results ---\n",
    "print(\"\\n--- Results ---\")\n",
    "print(\"Original signal dimension (n):\", n)\n",
    "print(\"Number of measurements (m):\", m)\n",
    "print(\"Number of channels:\", num_channels)\n",
    "print(\"Number of dictionary atoms (n_features_dict):\", n_features_dict)\n",
    "\n",
    "# Calculate Reconstruction Error (Normalized Mean Squared Error - NMSE)\n",
    "# NMSE = ||original - reconstructed||_F / ||original||_F\n",
    "# Here, original signal X = D_true @ Z_true\n",
    "# Reconstructed signal X_hat = D_hat @ Z_hat\n",
    "\n",
    "X_true = D_true @ Z_true\n",
    "X_hat = D_hat @ Z_hat\n",
    "\n",
    "nmse = np.linalg.norm(X_true - X_hat, 'fro') / np.linalg.norm(X_true, 'fro')\n",
    "print(f\"\\nNormalized Mean Squared Error (NMSE) for X: {nmse:.4f}\")\n",
    "\n",
    "# Check sparsity of Z_hat (number of non-zero rows)\n",
    "# A row is considered non-zero if its L2 norm is above a small threshold\n",
    "threshold_for_zero_row = 1e-6\n",
    "num_non_zero_rows_Z_true = np.sum(np.linalg.norm(Z_true, axis=1) > threshold_for_zero_row)\n",
    "num_non_zero_rows_Z_hat = np.sum(np.linalg.norm(Z_hat, axis=1) > threshold_for_zero_row)\n",
    "\n",
    "print(f\"Number of non-zero rows in Z_true: {num_non_zero_rows_Z_true} / {n_features_dict}\")\n",
    "print(f\"Number of non-zero rows in Z_hat: {num_non_zero_rows_Z_hat} / {n_features_dict}\")\n",
    "\n",
    "print(\"\\nShape of estimated D (Dictionary):\", D_hat.shape)\n",
    "print(\"Shape of estimated Z (Sparse Coefficients):\", Z_hat.shape)\n",
    "print(\"Shape of estimated P (Proxy for D):\", P_hat.shape)\n",
    "print(\"Shape of estimated Q (Proxy for Z):\", Q_hat.shape)\n",
    "\n",
    "# Optional: Visualize some results (requires matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.abs(Z_true), aspect='auto', cmap='viridis')\n",
    "plt.title('Absolute Z_true (Row-Sparse)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.abs(Z_hat), aspect='auto', cmap='viridis')\n",
    "plt.title('Absolute Z_hat (Estimated)')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80861407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_mat_data_from_bciiv(file_path: str):\n",
    "    mat_data = sio.loadmat(file_path, spmatrix=False)\n",
    "\n",
    "    # INT16 to uV\n",
    "    fs = int(mat_data[\"nfo\"][\"fs\"][0][0][0][0])\n",
    "    chs = np.array([arr[0] for arr in  mat_data[\"nfo\"][\"clab\"][0][0][0]])\n",
    "    X = 0.1 * np.array(mat_data[\"cnt\"], dtype=np.float64)\n",
    "\n",
    "    return chs, fs, X\n",
    "\n",
    "file_path = './../BCICIV_1calib_1000Hz_mat/BCICIV_calib_ds1a_1000Hz.mat'\n",
    "chs, fs, raw_signals = get_mat_data_from_bciiv(file_path)\n",
    "\n",
    "X = raw_signals\n",
    "\n",
    "chs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a952dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "class BCIIVLoader:\n",
    "    \"\"\"\n",
    "    Loader for BCI Competition IV .mat EEG files.\n",
    "    Allows channel selection and time slicing.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self._load_mat()\n",
    "    \n",
    "    def _load_mat(self):\n",
    "        mat_data = sio.loadmat(self.file_path, spmatrix=False)\n",
    "        self.fs = int(mat_data[\"nfo\"][\"fs\"][0][0][0][0])\n",
    "        self.ch_names = np.array([arr[0] for arr in mat_data[\"nfo\"][\"clab\"][0][0][0]])\n",
    "        self.data = 0.1 * np.array(mat_data[\"cnt\"], dtype=np.float64)  # shape: (samples, channels)\n",
    "        self.n_samples, self.n_channels = self.data.shape\n",
    "\n",
    "    def get_channel_data(self, channel):\n",
    "        \"\"\"\n",
    "        Returns the data for a specific channel (by name or index).\n",
    "        \"\"\"\n",
    "        if isinstance(channel, str):\n",
    "            idx = np.where(self.ch_names == channel)[0]\n",
    "            if len(idx) == 0:\n",
    "                raise ValueError(f\"Channel '{channel}' not found.\")\n",
    "            idx = idx[0]\n",
    "        else:\n",
    "            idx = int(channel)\n",
    "        return self.data[:, idx]\n",
    "\n",
    "    def get_data(self, channels=None, tmin=None, tmax=None):\n",
    "        \"\"\"\n",
    "        Returns data for selected channels and time range.\n",
    "        channels: list of channel names or indices (default: all)\n",
    "        tmin, tmax: time in seconds (default: full range)\n",
    "        \"\"\"\n",
    "        # Channel selection\n",
    "        if channels is None:\n",
    "            ch_idx = np.arange(self.n_channels)\n",
    "        else:\n",
    "            ch_idx = []\n",
    "            for ch in channels:\n",
    "                if isinstance(ch, str):\n",
    "                    idx = np.where(self.ch_names == ch)[0]\n",
    "                    if len(idx) == 0:\n",
    "                        raise ValueError(f\"Channel '{ch}' not found.\")\n",
    "                    ch_idx.append(idx[0])\n",
    "                else:\n",
    "                    ch_idx.append(int(ch))\n",
    "            ch_idx = np.array(ch_idx)\n",
    "        \n",
    "        # Time selection\n",
    "        start = 0 if tmin is None else int(self.fs * tmin)\n",
    "        end = self.n_samples if tmax is None else int(self.fs * tmax)\n",
    "\n",
    "        # Clamp to valid range\n",
    "        start = max(0, start)\n",
    "        end = min(self.n_samples, end)\n",
    "        \n",
    "        t = np.arange(start, end) / self.fs\n",
    "        X = self.data[start:end, ch_idx]\n",
    "    \n",
    "        # Return data [samples, channels], time vector and sampling frequency\n",
    "        return X, t, self.fs\n",
    "\n",
    "# Example usage:\n",
    "loader = BCIIVLoader('./../BCICIV_1calib_1000Hz_mat/BCICIV_calib_ds1a_1000Hz.mat')\n",
    "X, t, fs = loader.get_data(tmin=10, tmax=12)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "\n",
    "class KSVD:\n",
    "    \"\"\"\n",
    "    K-SVD dictionary learning algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int\n",
    "        Number of dictionary atoms.\n",
    "    max_iter : int\n",
    "        Number of K-SVD iterations.\n",
    "    sparsity : int\n",
    "        Target number of non-zero coefficients in sparse coding.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=40, max_iter=10, sparsity=5):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.sparsity = sparsity\n",
    "        self.dictionary_ = None\n",
    "\n",
    "    def _initialize_dictionary(self, X):\n",
    "        # Initialize dictionary with random normalized columns from data\n",
    "        n_features, n_samples = X.shape\n",
    "        indices = np.random.choice(n_samples, self.n_components, replace=False)\n",
    "        D = X[:, indices].copy()\n",
    "        D /= np.linalg.norm(D, axis=0, keepdims=True) + 1e-8\n",
    "        return D\n",
    "\n",
    "    def _sparse_coding(self, X, D):\n",
    "        # Compute sparse codes using OMP\n",
    "        omp = OrthogonalMatchingPursuit(n_nonzero_coefs=self.sparsity)\n",
    "        Gamma = np.zeros((self.n_components, X.shape[1]))\n",
    "        for i in range(X.shape[1]):\n",
    "            omp.fit(D, X[:, i])\n",
    "            Gamma[:, i] = omp.coef_\n",
    "        return Gamma\n",
    "\n",
    "    def _dictionary_update(self, X, D, Gamma):\n",
    "        # Update each atom\n",
    "        for k in range(self.n_components):\n",
    "            # Find signals that use atom k\n",
    "            omega = np.nonzero(Gamma[k, :])[0]\n",
    "            if len(omega) == 0:\n",
    "                continue\n",
    "            # Compute the residual without atom k\n",
    "            E = X[:, omega] - D @ Gamma[:, omega] + np.outer(D[:, k], Gamma[k, omega])\n",
    "            # SVD on the residual\n",
    "            U, S, Vt = svd(E, full_matrices=False)\n",
    "            # Update atom k and coefficients\n",
    "            D[:, k] = U[:, 0]\n",
    "            Gamma[k, omega] = S[0] * Vt[0, :]\n",
    "        return D, Gamma\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Learn dictionary from data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_features, n_samples)\n",
    "            Training data matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        X = np.array(X, dtype=float)\n",
    "        # Initialize\n",
    "        self.dictionary_ = self._initialize_dictionary(X)\n",
    "\n",
    "        for it in range(self.max_iter):\n",
    "            # Sparse coding step\n",
    "            Gamma = self._sparse_coding(X, self.dictionary_)\n",
    "            # Dictionary update step\n",
    "            self.dictionary_, Gamma = self._dictionary_update(X, self.dictionary_, Gamma)\n",
    "            # (Optional) compute reconstruction error\n",
    "            error = np.linalg.norm(X - self.dictionary_ @ Gamma)**2\n",
    "            print(f\"Iteration {it+1}/{self.max_iter}, reconstruction error: {error:.4f}\")\n",
    "\n",
    "        self.code_ = Gamma\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Encode data using the learned dictionary.\n",
    "        \"\"\"\n",
    "        return self._sparse_coding(np.array(X, dtype=float), self.dictionary_)\n",
    "\n",
    "    def reconstruct(self):\n",
    "        \"\"\"\n",
    "        Reconstruct signals from codes and dictionary.\n",
    "        \"\"\"\n",
    "        return self.dictionary_ @ self.code_\n",
    "\n",
    "\n",
    "# Generate synthetic data: signals composed of random combinations of sinusoids\n",
    "t = np.linspace(0, 1, 256)\n",
    "S = np.vstack([np.sin(2*np.pi*f*t) for f in [5, 15, 30]])\n",
    "X = (np.random.randn(3, 100).T @ S).T  # shape (256,100)\n",
    "\n",
    "ksvd = KSVD(max_iter=15, sparsity=3)\n",
    "ksvd.fit(X)\n",
    "codes = ksvd.transform(X)\n",
    "X_hat = ksvd.reconstruct()\n",
    "print(\"Reconstruction done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e055ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "from sklearn.linear_model import Lasso # For reconstruction (alternative to SPGL1 BPDN)\n",
    "from scipy.fft import dct, idct # For potential Gabor-like dictionary comparison\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# import mne # Uncomment if you have mne-python installed for real EEG data loading\n",
    "\n",
    "# --- 1. Configuration Parameters (Based on Paper) ---\n",
    "# EEG Data Parameters\n",
    "SAMPLING_FREQUENCY = 256 # Hz [cite: 59]\n",
    "EPOCH_LENGTH_SECONDS = 4 # seconds [cite: 69]\n",
    "N_SAMPLES_PER_EPOCH = EPOCH_LENGTH_SECONDS * SAMPLING_FREQUENCY # 1024 samples\n",
    "\n",
    "# K-SVD Parameters\n",
    "TRAINING_SET_SIZE = 8000 # EEG segments [cite: 64]\n",
    "TEST_SET_SIZE = 3200 # EEG segments [cite: 64calculate_nmse]\n",
    "DICTIONARY_SIZE = 3000 # Number of atoms [cite: 81]\n",
    "MAX_COEFFICIENTS_T0 = 60 # Number of non-zero coefficients per atom [cite: 86]\n",
    "K_SVD_MAX_ITERATIONS = 80 # [cite: 72]\n",
    "\n",
    "# Compressed Sensing Parameters\n",
    "COMPRESSION_RATIOS = [2, 4, 8] # [cite: 72]\n",
    "# Reconstruction algorithm: Paper uses SPGL1 BPDN. We'll use sklearn.linear_model.Lasso as an approximation.\n",
    "# For exact BPDN, you'd need to integrate SPGL1 or a similar convex optimization solver (e.g., cvxpy).\n",
    "\n",
    "# --- 2. Helper Functions ---\n",
    "\n",
    "def generate_eeg_data_placeholder(num_segments, segment_length):\n",
    "    \"\"\"\n",
    "    Generates placeholder EEG-like data for demonstration.\n",
    "    In a real scenario, this would load and preprocess actual EDF files.\n",
    "    \"\"\"\n",
    "    print(f\"Generating {num_segments} placeholder EEG segments of length {segment_length}...\")\n",
    "    # Simulate some oscillatory patterns with noise\n",
    "    data = np.zeros((num_segments, segment_length))\n",
    "    for i in range(num_segments):\n",
    "        t = np.linspace(0, EPOCH_LENGTH_SECONDS, segment_length, endpoint=False)\n",
    "        # Combine a few sinusoids and noise\n",
    "        signal = 5 * np.sin(2 * np.pi * 5 * t) + \\\n",
    "                 2 * np.sin(2 * np.pi * 12 * t + np.pi/4) + \\\n",
    "                 1 * np.sin(2 * np.pi * 30 * t) + \\\n",
    "                 np.random.normal(0, 0.5, segment_length)\n",
    "        data[i, :] = signal\n",
    "    print(\"Placeholder data generation complete.\")\n",
    "    return data\n",
    "\n",
    "def create_random_sensing_matrix(M, N):\n",
    "    \"\"\"\n",
    "    Creates a white-noise Gaussian random matrix (Phi).\n",
    "    M: compressed signal length\n",
    "    N: original signal length\n",
    "    \"\"\"\n",
    "    print(f\"Creating random sensing matrix Phi of shape ({M}, {N})...\")\n",
    "    # Paper uses white-noise Gaussian random matrix [cite: 72]\n",
    "    return np.random.randn(M, N)\n",
    "\n",
    "def calculate_nmse(original_signal, reconstructed_signal):\n",
    "    \"\"\"\n",
    "    Calculates the Normalized Mean Square Error (NMSE) between two signals.\n",
    "    NMSE(x, x_hat) = ||x - x_hat||_2 / ||x - mean(x)||_2 [cite: 74]\n",
    "    \"\"\"\n",
    "    original_signal = np.asarray(original_signal)\n",
    "    reconstructed_signal = np.asarray(reconstructed_signal)\n",
    "\n",
    "    # Ensure signals are 1D arrays for norm calculation\n",
    "    original_signal_flat = original_signal.flatten()\n",
    "    reconstructed_signal_flat = reconstructed_signal.flatten()\n",
    "\n",
    "    # Calculate numerator: ||x - x_hat||_2\n",
    "    numerator = np.linalg.norm(original_signal_flat - reconstructed_signal_flat)\n",
    "\n",
    "    # Calculate denominator: ||x - mean(x)||_2\n",
    "    denominator = np.linalg.norm(original_signal_flat - np.mean(original_signal_flat))\n",
    "\n",
    "    if denominator == 0:\n",
    "        return float('inf') # Avoid division by zero if original signal is constant\n",
    "    return numerator / denominator # [cite: 74]\n",
    "\n",
    "# --- 3. Main K-SVD Dictionary Learning Process ---\n",
    "\n",
    "def learn_ksvd_dictionary(training_data, n_atoms, max_coeffs, n_iterations):\n",
    "    \"\"\"\n",
    "    Learns a K-SVD dictionary using MiniBatchDictionaryLearning from sklearn.\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting K-SVD dictionary learning with {n_atoms} atoms and {max_coeffs} max coefficients...\")\n",
    "    print(f\"Training data shape: {training_data.shape}\")\n",
    "\n",
    "    # MiniBatchDictionaryLearning is an efficient variant of K-SVD\n",
    "    # n_components: number of atoms in the dictionary\n",
    "    # transform_n_nonzero_coeffs: T0, max number of non-zero coefficients [cite: 46]\n",
    "    # n_iter: max number of iterations [cite: 72]\n",
    "    # random_state for reproducibility\n",
    "    # batch_size: can be tuned for performance, smaller batches can lead to faster convergence\n",
    "    # alpha: L1 regularization strength for sparse coding (implicitly in OMP, but sklearn uses it)\n",
    "    ksvd_learner = MiniBatchDictionaryLearning(\n",
    "        n_components=n_atoms,\n",
    "        transform_n_nonzero_coefs=max_coeffs,\n",
    "        # n_iter=n_iterations,\n",
    "        random_state=42, # For reproducibility\n",
    "        batch_size=256, # A common batch size, can be adjusted\n",
    "        alpha=1.0 # Default alpha for sparse coding (L1 regularization)\n",
    "    )\n",
    "\n",
    "    # Fit the model to the training data to learn the dictionary\n",
    "    ksvd_learner.fit(training_data)\n",
    "    learned_dictionary = ksvd_learner.components_\n",
    "    print(f\"K-SVD dictionary learning complete. Dictionary shape: {learned_dictionary.shape}\")\n",
    "    return learned_dictionary\n",
    "\n",
    "# --- 4. Compressed Sensing and Reconstruction ---\n",
    "\n",
    "def perform_compressed_sensing_and_reconstruction(\n",
    "    test_data,\n",
    "    learned_dictionary,\n",
    "    compression_ratio,\n",
    "    reconstruction_method='lasso' # 'lasso' for sklearn.Lasso\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs compressed sensing and reconstruction on test data using the learned dictionary.\n",
    "    \"\"\"\n",
    "    N = test_data.shape[1] # Original signal length [cite: 74]\n",
    "    M = int(N / compression_ratio) # Compressed signal length [cite: 74]\n",
    "\n",
    "    if M == 0:\n",
    "        raise ValueError(f\"Compression ratio {compression_ratio} is too high for N={N}. M cannot be zero.\")\n",
    "\n",
    "    # Generate sensing matrix Phi [cite: 72]\n",
    "    Phi = create_random_sensing_matrix(M, N)\n",
    "\n",
    "    reconstructed_signals = []\n",
    "    nmse_values = []\n",
    "    computation_times = []\n",
    "\n",
    "    print(f\"\\nStarting CS reconstruction for CR={compression_ratio}:1 (M={M})...\")\n",
    "    for i, original_signal in enumerate(test_data):\n",
    "        if i % (TEST_SET_SIZE // 10) == 0 and i > 0:\n",
    "            print(f\"  Processing test signal {i}/{TEST_SET_SIZE}...\")\n",
    "\n",
    "        # Step 1: Compress the signal y = Phi * x [cite: 25, 26]\n",
    "        y_compressed = np.dot(Phi, original_signal)\n",
    "\n",
    "        # Step 2: Reconstruction (solving y = Phi * D * theta for theta)\n",
    "        # This is the core of the BPDN problem.\n",
    "        # Paper uses SPGL1. Here, we approximate with Lasso (L1 regularization).\n",
    "        # We need to solve: min ||theta||_1 subject to ||y - (Phi @ D) @ theta||_2 <= epsilon\n",
    "        effective_sensing_matrix_sparse_domain = np.dot(Phi, learned_dictionary.T) # (M, P)\n",
    "\n",
    "        # Use Lasso for sparse coefficient recovery\n",
    "        # The 'alpha' parameter in Lasso controls the strength of L1 regularization.\n",
    "        # A smaller alpha will favor less sparsity, larger alpha more sparsity.\n",
    "        # This needs careful tuning to approximate BPDN behavior.\n",
    "        start_time = plt.rcParams['figure.subplot.right'] if 'figure.subplot.right' in plt.rcParams else 0 # Just a placeholder for time measurement start\n",
    "        reconstruction_lasso = Lasso(alpha=0.001, # Needs tuning! Corresponds to epsilon/noise level in BPDN\n",
    "                                     fit_intercept=False,\n",
    "                                     max_iter=10000, # Increase if convergence issues\n",
    "                                     tol=1e-4) # Tolerance for convergence\n",
    "        try:\n",
    "            reconstruction_lasso.fit(effective_sensing_matrix_sparse_domain, y_compressed)\n",
    "            sparse_coefficients_reconstructed = reconstruction_lasso.coef_\n",
    "        except Exception as e:\n",
    "            print(f\"Lasso reconstruction failed for signal {i}: {e}. Skipping this signal.\")\n",
    "            continue\n",
    "        end_time = plt.rcParams['figure.subplot.left'] if 'figure.subplot.left' in plt.rcParams else 0 # Just a placeholder for time measurement end\n",
    "        computation_times.append(end_time - start_time) # Placeholder for actual time measurement\n",
    "\n",
    "        # Step 3: Reconstruct the original signal: x_hat = D * theta_hat [cite: 22]\n",
    "        x_reconstructed = np.dot(learned_dictionary.T, sparse_coefficients_reconstructed) # Transpose dictionary if atoms are rows\n",
    "\n",
    "        reconstructed_signals.append(x_reconstructed)\n",
    "        nmse_values.append(calculate_nmse(original_signal, x_reconstructed))\n",
    "\n",
    "    mean_nmse = np.mean(nmse_values) if nmse_values else float('nan')\n",
    "    mean_comp_time = np.mean(computation_times) if computation_times else float('nan')\n",
    "    print(f\"  CS Reconstruction complete for CR={compression_ratio}:1. Mean NMSE: {mean_nmse:.4f}\")\n",
    "    print(f\"  Mean Computation Time (Approx): {mean_comp_time:.4f} seconds.\")\n",
    "    return mean_nmse, mean_comp_time\n",
    "\n",
    "# --- 5. Gabor Dictionary (for comparison, as mentioned in paper) ---\n",
    "\n",
    "def create_gabor_dictionary(N, num_freqs, num_shifts):\n",
    "    \"\"\"\n",
    "    Creates a Gabor dictionary for a given signal length N.\n",
    "    Simplified Gabor atom generation. A more rigorous implementation would use\n",
    "    specific Gabor wavelets parameters (center frequency, bandwidth, time shift).\n",
    "    \"\"\"\n",
    "    print(f\"Creating Gabor dictionary with N={N}...\")\n",
    "    gabor_atoms = []\n",
    "    t = np.linspace(0, 1, N, endpoint=False) # Time vector (normalized)\n",
    "\n",
    "    for freq_idx in range(num_freqs):\n",
    "        freq = 1 + freq_idx * (SAMPLING_FREQUENCY / 2 / num_freqs) # Example: covering up to Nyquist\n",
    "        for shift_idx in range(num_shifts):\n",
    "            shift = shift_idx * (N / num_shifts)\n",
    "            # Simple Gabor atom: sine wave modulated by Gaussian\n",
    "            # A more accurate Gabor uses complex exponentials and includes bandwidth/sigma\n",
    "            gaussian_window = np.exp(-0.5 * ((t * N - shift) / (N / 10))**2) # Example Gaussian spread\n",
    "            gabor_atom = np.sin(2 * np.pi * freq * t) * gaussian_window\n",
    "            gabor_atom /= np.linalg.norm(gabor_atom) # Normalize\n",
    "            gabor_atoms.append(gabor_atom)\n",
    "\n",
    "    gabor_dict = np.array(gabor_atoms)\n",
    "    print(f\"Gabor dictionary created. Shape: {gabor_dict.shape}\")\n",
    "    return gabor_dict\n",
    "\n",
    "# --- 6. Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42) # Set random seed for reproducibility\n",
    "\n",
    "    # --- Data Generation/Loading ---\n",
    "    # In a real scenario, you would load your EEG data here:\n",
    "    # Example using MNE-Python (requires installation and EDF files)\n",
    "    # raw = mne.io.read_raw_edf('path/to/chb01_01.edf', preload=True)\n",
    "    # raw.resample(sfreq=SAMPLING_FREQUENCY)\n",
    "    # full_eeg_data = raw.get_data() # (n_channels, n_samples)\n",
    "    #\n",
    "    # # Assuming a single channel or flattening multiple channels for simplicity\n",
    "    # # For multi-channel EEG, you might learn a dictionary for each channel or a joint dictionary.\n",
    "    # single_channel_data = full_eeg_data[0, :]\n",
    "    #\n",
    "    # # Segment the data\n",
    "    # all_segments = []\n",
    "    # for i in range(0, single_channel_data.shape[0] - N_SAMPLES_PER_EPOCH + 1, N_SAMPLES_PER_EPOCH):\n",
    "    #     all_segments.append(single_channel_data[i:i+N_SAMPLES_PER_EPOCH])\n",
    "    # all_segments = np.array(all_segments)\n",
    "    #\n",
    "    # # Split into training and testing (ensure no overlap as per paper [cite: 63])\n",
    "    # np.random.shuffle(all_segments) # Shuffle to ensure random selection if not pre-shuffled\n",
    "    # training_data_all_channels = all_segments[:TRAINING_SET_SIZE]\n",
    "    # test_data_all_channels = all_segments[TRAINING_SET_SIZE : TRAINING_SET_SIZE + TEST_SET_SIZE]\n",
    "\n",
    "    # For demonstration, generate placeholder data:\n",
    "    training_data = generate_eeg_data_placeholder(TRAINING_SET_SIZE, N_SAMPLES_PER_EPOCH)\n",
    "    test_data = generate_eeg_data_placeholder(TEST_SET_SIZE, N_SAMPLES_PER_EPOCH)\n",
    "\n",
    "    print(training_data.shape)\n",
    "\n",
    "    # Normalize data (important for many dictionary learning algorithms)\n",
    "    scaler = StandardScaler()\n",
    "    training_data_scaled = scaler.fit_transform(training_data)\n",
    "    test_data_scaled = scaler.transform(test_data) # Use same scaler as training\n",
    "\n",
    "    # --- K-SVD Dictionary Learning ---\n",
    "    learned_ksvd_dictionary = learn_ksvd_dictionary(\n",
    "        training_data_scaled,\n",
    "        100, # DICTIONARY_SIZE, # 3000 atoms [cite: 81]\n",
    "        MAX_COEFFICIENTS_T0, # 60 coeffs [cite: 86]\n",
    "        K_SVD_MAX_ITERATIONS # 80 iterations [cite: 72]\n",
    "    )\n",
    "\n",
    "    # # --- Evaluate K-SVD Performance ---\n",
    "    # print(\"\\n--- Evaluating K-SVD Dictionary Performance ---\")\n",
    "    # ksvd_nmse_results = []\n",
    "    # ksvd_comp_time_results = []\n",
    "    # for cr in COMPRESSION_RATIOS:\n",
    "    #     nmse, comp_time = perform_compressed_sensing_and_reconstruction(\n",
    "    #         test_data_scaled,\n",
    "    #         learned_ksvd_dictionary,\n",
    "    #         cr,\n",
    "    #         reconstruction_method='lasso'\n",
    "    #     )\n",
    "    #     ksvd_nmse_results.append(nmse)\n",
    "    #     ksvd_comp_time_results.append(comp_time)\n",
    "\n",
    "    # print(\"\\nK-SVD NMSE Results per CR:\", ksvd_nmse_results)\n",
    "    # print(\"K-SVD Comp Time Results per CR:\", ksvd_comp_time_results)\n",
    "\n",
    "    # # --- Gabor Dictionary Comparison (as mentioned in paper) ---\n",
    "    # # The paper uses a Gabor dictionary of 15938 atoms [cite: 85]\n",
    "    # # Creating a Gabor dictionary of this size requires careful parameterization.\n",
    "    # # Here's a conceptual example.\n",
    "    # print(\"\\n--- Evaluating Gabor Dictionary Performance (for comparison) ---\")\n",
    "    # # For a real Gabor, you need specific time/frequency resolutions.\n",
    "    # # N=1024, if 15938 atoms is the goal, need to set num_freqs and num_shifts accordingly.\n",
    "    # # Example: 128 freqs * 128 shifts ~ 16000 atoms\n",
    "    # # A more precise Gabor implementation would be needed for direct comparison.\n",
    "    # try:\n",
    "    #     gabor_dictionary = create_gabor_dictionary(\n",
    "    #         N_SAMPLES_PER_EPOCH,\n",
    "    #         num_freqs=128, # Example: for ~16000 atoms\n",
    "    #         num_shifts=128  # Example: for ~16000 atoms\n",
    "    #     )\n",
    "\n",
    "    #     gabor_nmse_results = []\n",
    "    #     gabor_comp_time_results = []\n",
    "    #     for cr in COMPRESSION_RATIOS:\n",
    "    #         nmse, comp_time = perform_compressed_sensing_and_reconstruction(\n",
    "    #             test_data_scaled,\n",
    "    #             gabor_dictionary,\n",
    "    #             cr,\n",
    "    #             reconstruction_method='lasso'\n",
    "    #         )\n",
    "    #         gabor_nmse_results.append(nmse)\n",
    "    #         gabor_comp_time_results.append(comp_time)\n",
    "\n",
    "    #     print(\"\\nGabor NMSE Results per CR:\", gabor_nmse_results)\n",
    "    #     print(\"Gabor Comp Time Results per CR:\", gabor_comp_time_results)\n",
    "\n",
    "    #     # --- Plotting Results (Similar to Fig. 3 & 4 in paper) ---\n",
    "    #     plt.figure(figsize=(12, 5))\n",
    "\n",
    "    #     # NMSE Plot\n",
    "    #     plt.subplot(1, 2, 1)\n",
    "    #     plt.plot([f\"{cr}:1\" for cr in COMPRESSION_RATIOS], ksvd_nmse_results, '-*', label='K-SVD dic', color='red')\n",
    "    #     plt.plot([f\"{cr}:1\" for cr in COMPRESSION_RATIOS], gabor_nmse_results, '-o', label='Gabor dic', color='blue')\n",
    "    #     plt.xlabel('Compression Ratio')\n",
    "    #     plt.ylabel('NMSE')\n",
    "    #     plt.title('NMSE K-SVD vs. Gabor Dictionary')\n",
    "    #     plt.legend()\n",
    "    #     plt.grid(True)\n",
    "\n",
    "    #     # Computation Cost Plot\n",
    "    #     plt.subplot(1, 2, 2)\n",
    "    #     plt.plot([f\"{cr}:1\" for cr in COMPRESSION_RATIOS], ksvd_comp_time_results, '-*', label='K-SVD dic', color='red')\n",
    "    #     plt.plot([f\"{cr}:1\" for cr in COMPRESSION_RATIOS], gabor_comp_time_results, '-o', label='Gabor dic', color='blue')\n",
    "    #     plt.xlabel('Compression Ratio')\n",
    "    #     plt.ylabel('Seconds')\n",
    "    #     plt.title('Computation Cost K-SVD vs. Gabor Dictionary')\n",
    "    #     plt.legend()\n",
    "    #     plt.grid(True)\n",
    "\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print(f\"\\nSkipping Gabor dictionary comparison due to error: {e}\")\n",
    "    #     print(\"Ensure 'num_freqs' and 'num_shifts' are set appropriately for a Gabor dictionary of comparable size.\")\n",
    "    #     print(\"The provided Gabor dictionary generation is a simplified conceptual example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b748f2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dict_learning] ....................Mean NMSE on test set: 1.2889\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import DictionaryLearning\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Load or prepare your EEG data:\n",
    "#    X_train: array of shape (n_train_segments, segment_length)\n",
    "#    X_test : array of shape (n_test_segments,  segment_length)\n",
    "#    e.g. from CHB-MIT: segment_length = 4 s × 256 Hz = 1024 samples\n",
    "# ------------------------------------------------------------------------------\n",
    "# For demo, let’s make synthetic data:\n",
    "n_train, n_test = 100, 3200\n",
    "segment_length = 4 * 256  # 4 s @256 Hz\n",
    "# e.g. X_train = load_eeg_segments(...)\n",
    "# Here random:\n",
    "X_train = np.random.randn(n_train, segment_length)\n",
    "X_test  = np.random.randn(n_test,  segment_length)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Learn a dictionary with scikit-learn\n",
    "# ------------------------------------------------------------------------------\n",
    "n_atoms    = 100      # number of dictionary atoms\n",
    "sparsity   = 30        # target nonzeros per segment\n",
    "max_iter   = 20        # K-SVD/online iterations\n",
    "\n",
    "dl = DictionaryLearning(\n",
    "    n_components=n_atoms,\n",
    "    transform_n_nonzero_coefs=sparsity,\n",
    "    fit_algorithm='lars',      # MOD-style update; very similar to K-SVD in practice\n",
    "    transform_algorithm='omp', # Orthogonal Matching Pursuit for sparse coding\n",
    "    max_iter=max_iter,\n",
    "    verbose=True,\n",
    "    random_state=0\n",
    ")\n",
    "# Note: sklearn expects X of shape (n_samples, n_features)\n",
    "# so here n_samples = n_train, n_features = segment_length\n",
    "D_components = dl.fit(X_train).components_   # shape (n_atoms, segment_length)\n",
    "D = D_components.T                          # shape (segment_length, n_atoms)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Build a random sensing matrix Φ and effective dictionary A = Φ·D\n",
    "# ------------------------------------------------------------------------------\n",
    "CR = 4  # compression ratio N/M\n",
    "N  = segment_length\n",
    "M  = N // CR\n",
    "Φ  = np.random.randn(M, N) / np.sqrt(M)      # normalized Gaussian\n",
    "\n",
    "A = Φ @ D  # shape (M, n_atoms)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. Reconstruct test segments via OMP on (A, y)\n",
    "# ------------------------------------------------------------------------------\n",
    "omp = OrthogonalMatchingPursuit(n_nonzero_coefs=sparsity)\n",
    "reconstruction_errors = []\n",
    "\n",
    "X_recon = np.zeros_like(X_test)\n",
    "for i, x_true in enumerate(X_test):\n",
    "    y = Φ @ x_true                           # compressed measurements\n",
    "    omp.fit(A, y)\n",
    "    α = omp.coef_                            # sparse code (length n_atoms)\n",
    "    x_hat = D @ α                            # reconstruct in signal space\n",
    "    X_recon[i] = x_hat\n",
    "    reconstruction_errors.append(np.linalg.norm(x_true - x_hat)**2)\n",
    "\n",
    "mean_nmse = np.mean(reconstruction_errors) / np.mean(np.linalg.norm(X_test, axis=1)**2)\n",
    "print(f\"Mean NMSE on test set: {mean_nmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc118512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.34115605e-05, -1.31800852e-05, -2.29188701e-05, ...,\n",
       "        -1.25283447e-04, -1.12009883e-04, -1.15086609e-04],\n",
       "       [-3.45223495e-05, -5.56788801e-05, -4.49851122e-05, ...,\n",
       "         1.52313879e-05,  1.05455216e-05,  2.25462109e-05],\n",
       "       [-2.23602284e-05, -3.42847510e-05, -2.72598235e-05, ...,\n",
       "        -2.04800907e-05, -2.32271284e-05, -2.01397373e-05],\n",
       "       ...,\n",
       "       [ 4.25726923e-06,  1.07125274e-05,  2.51657876e-05, ...,\n",
       "         3.76446703e-05,  1.91701990e-05, -1.85331468e-05],\n",
       "       [ 1.82054427e-05,  6.53294523e-06,  1.46664068e-05, ...,\n",
       "         2.53890719e-05,  3.81875524e-05,  4.82226537e-05],\n",
       "       [ 4.96500858e-05,  4.62652006e-05,  3.56947840e-05, ...,\n",
       "        -1.31065050e-05, -5.82600441e-06, -2.16787790e-05]],\n",
       "      shape=(10, 512))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import mne\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from scipy.signal import resample\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "class Loader(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for data loaders.\n",
    "    This class defines the interface for loading datasets.\n",
    "    \"\"\"\n",
    "    _id: int = time.time_ns()  # Unique identifier for the loader instance\n",
    "    _dataset: str\n",
    "    _fs: float\n",
    "    _ch_names: list[str]\n",
    "    _data: np.ndarray # (n_blocks, n_samples, n_channels)\n",
    "    _n_samples: int\n",
    "    _n_channels: int\n",
    "    # Number of blocks of segments in the dataset\n",
    "    _n_blocks: int\n",
    "    _segment_length_sec: float\n",
    "\n",
    "    def __init__(self,  n_blocks: int, segment_length_sec: float, random_state: int = 42):\n",
    "        self._n_blocks = n_blocks\n",
    "        self._segment_length_sec = segment_length_sec\n",
    "\n",
    "        self._load_data(n_blocks, segment_length_sec, random_state)\n",
    "\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _load_data(self, n_blocks: int, segment_length_sec: float, random_state: int):\n",
    "        ...\n",
    "\n",
    "    def get_random_segments(self, n_segments: int, random_state: int = 42) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns a list of individual segments randomly selected from the dataset.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(random_state)\n",
    "\n",
    "        total_n_segments = self.n_blocks * self.n_channels\n",
    "\n",
    "        if n_segments > total_n_segments:\n",
    "            raise ValueError(f\"Requested {n_segments} segments, but only {total_n_segments} available.\")\n",
    "\n",
    "        # Generate random indices for blocks and channels\n",
    "        indices = rng.choice(total_n_segments, size=n_segments, replace=False)\n",
    "\n",
    "        # Map indices to block and channel\n",
    "        segments = []\n",
    "        for idx in indices:\n",
    "            block_idx = idx // self.n_channels\n",
    "            channel_idx = idx % self.n_channels\n",
    "            segments.append(self._data[block_idx, :, channel_idx])\n",
    "\n",
    "        return np.array(segments)\n",
    "\n",
    "    def resample(self, new_fs: float):\n",
    "        \"\"\"\n",
    "        Resample the data to a new sampling frequency.\n",
    "        Assumes self._data has shape (n_blocks, n_samples, n_channels).\n",
    "        \"\"\"\n",
    "        if self._fs == new_fs:\n",
    "            return\n",
    "        if new_fs <= 0:\n",
    "            raise ValueError(\"New sampling frequency must be positive.\")\n",
    "\n",
    "        new_n_samples = int(np.round(self.n_samples * new_fs / self._fs))\n",
    "        # Resample each block independently along the samples axis\n",
    "        self._data = resample(self._data, new_n_samples, axis=1)\n",
    "        self._n_samples = new_n_samples\n",
    "        self._fs = new_fs\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file_path: str):\n",
    "        \"\"\"\n",
    "        Load data from a file.\n",
    "        This method should be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File {file_path} does not exist.\")\n",
    "\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "            if not isinstance(obj, cls):\n",
    "                raise TypeError(f\"Expected object of type {cls.__name__}, got {type(obj).__name__}.\")\n",
    "            \n",
    "            return obj\n",
    "\n",
    "    def save(self, folder_path: str):\n",
    "        file_path = os.path.join(folder_path, f\"{self.id}_{self.dataset}_fs_{self.fs}_blocks_{self.n_blocks}.pkl\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @property\n",
    "    def id(self) -> int:\n",
    "        return self._id\n",
    "\n",
    "    @property\n",
    "    def dataset(self) -> str:\n",
    "        return self._dataset\n",
    "\n",
    "    @property\n",
    "    def fs(self) -> float:\n",
    "        return self._fs\n",
    "\n",
    "    @property\n",
    "    def ch_names(self) -> list[str]:\n",
    "        return self._ch_names\n",
    "\n",
    "    @property\n",
    "    def data(self) -> np.ndarray:\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def n_samples(self) -> int:\n",
    "        return self._n_samples\n",
    "    \n",
    "    @property\n",
    "    def n_channels(self) -> int:\n",
    "        return self._n_channels\n",
    "    \n",
    "    @property\n",
    "    def n_blocks(self) -> int:\n",
    "        return self._n_blocks\n",
    "    \n",
    "    @property\n",
    "    def segment_length_sec(self) -> float:\n",
    "        return self._segment_length_sec\n",
    "\n",
    "\n",
    "class CHBMITLoader(Loader):\n",
    "    \"\"\"\n",
    "    Loader for CHB-MIT Scalp EEG Database.\n",
    "    \"\"\"\n",
    "\n",
    "    _dataset = \"chbmit\"\n",
    "    _fs = 256.0  # Default sampling frequency for CHB-MIT\n",
    "    _ch_names = [\n",
    "        'FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1',\n",
    "        'FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1',\n",
    "        'FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2',\n",
    "        'FP2-F8', 'F8-T8', 'T8-P8', 'P8-O2',\n",
    "        'FZ-CZ', 'CZ-PZ', 'P7-T7', 'T7-FT9',\n",
    "        'FT9-FT10', 'FT10-T8'\n",
    "    ]\n",
    "    _n_channels = len(_ch_names)\n",
    "\n",
    "\n",
    "    def _load_data(self, n_blocks: int, segment_length_sec: float, random_state: int):\n",
    "        \"\"\"\n",
    "        Load EEG data from the CHB-MIT dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_blocks : int\n",
    "            Number of blocks to load.\n",
    "        segment_length_sec : float\n",
    "            Length of each segment in seconds.\n",
    "        random_state : int\n",
    "            Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        # Set random seed\n",
    "        rng = np.random.default_rng(random_state)\n",
    "\n",
    "        # Define the root directory for CHB-MIT dataset\n",
    "        root_dir = f\"{cwd}/../../files/CHBMIT\"\n",
    "\n",
    "        # Get all EDF files from all subject folders\n",
    "        edf_files = sorted(glob.glob(os.path.join(root_dir, \"chb*/\", \"*.edf\")))\n",
    "        if not edf_files:\n",
    "            raise FileNotFoundError(f\"No EDF files found in {root_dir}.\")\n",
    "\n",
    "        # Shuffle the list of EDF files\n",
    "        rng.shuffle(edf_files)\n",
    "\n",
    "        # Initialize data storage\n",
    "        blocks = []\n",
    "        n_samples_per_segment = int(segment_length_sec * self._fs)\n",
    "\n",
    "        for edf_file in edf_files:\n",
    "            edf_file_name = os.path.basename(edf_file)\n",
    "            # Read the EDF file using MNE\n",
    "            raw = mne.io.read_raw_edf(edf_file, include=self.ch_names, verbose=False, )\n",
    "            # Exclude last channel (duplicated T8-P8) \n",
    "            data = raw.get_data().T[:, :-1] # (n_samples, n_channels)\n",
    "            if data.shape[1] != self.n_channels:\n",
    "                continue  # Skip files with unexpected channel count\n",
    "\n",
    "            max_start_idx = data.shape[0] - n_samples_per_segment\n",
    "            if max_start_idx <= 0:\n",
    "                continue\n",
    "\n",
    "            possible_starts = np.arange(0, max_start_idx + 1, n_samples_per_segment)\n",
    "            initial_idx = rng.choice(possible_starts.size)\n",
    "            for start in possible_starts[initial_idx:]:\n",
    "                end = start + n_samples_per_segment\n",
    "                block = data[start:end, :]\n",
    "                blocks.append(block)\n",
    "    \n",
    "                if len(blocks) >= n_blocks:\n",
    "                    break\n",
    "\n",
    "            if len(blocks) >= n_blocks:\n",
    "                break\n",
    "        \n",
    "        if len(blocks) < n_blocks:\n",
    "            # TODO: Handle case where not enough blocks are loaded. Save the initial_idx of each edf_file and if there is not enough blocks, shuffle the list of edf_files again, iterate over each of them and get new initial_idx before the one used earlier.\n",
    "            raise ValueError(f\"Requested {n_blocks} blocks, but only {len(blocks)} were loaded.\")\n",
    "\n",
    "        self._data = np.array(blocks)  # Shape (n_blocks, n_samples_per_segment, n_channels)\n",
    "        self._n_samples = n_samples_per_segment\n",
    "\n",
    "# chbmitLoader = CHBMITLoader(n_blocks=11200, segment_length_sec=4.0)\n",
    "chbmitLoader = CHBMITLoader.load(f\"{cwd}/../../files/processed/1748879687102909305_chbmit_fs_128_blocks_11200.pkl\")\n",
    "# chbmitLoader.resample(new_fs=128) \n",
    "chbmitLoader.data\n",
    "chbmitLoader.get_random_segments(n_segments=10)\n",
    "# chbmitLoader.save(folder_path=f\"{cwd}/../../files/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3a8d4a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
